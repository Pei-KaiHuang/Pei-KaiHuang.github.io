
<!DOCTYPE HTML>

<html>

<head>
	<title>Pei-Kai Huang</title>
	<meta charset="utf-8" />        
    <meta name="AUTHOR" content="Pei-Kai Huang">
	<meta name="keywords" content="Pei-Kai Huang, deep learning, computer vision, 黄培凯" />
	<meta name="description" content="Peikai Huang (Pei-Kai Huang, 黄培凯)" />
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<link rel="stylesheet" href="jemdoc.css" type="text/css" />
	<link rel="shortcut icon" href="logo.ico">
	<style>
		table{
			border: none;
		}
		table td{
			border: none;
		}
	</style>
</head>
    
 <script type="text/javascript" async="" src="assets/ga.js"></script>




<body bgcolor="#EEEEEE">

<div >

<div id="layout-content">
<div id="toptitle">
<h1>Peikai Huang (Pei-Kai Huang, 黄培凯)</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://pei-kaihuang.github.io/"><img src="picture/pk_daughter.jpg" alt="alt text" width="150px" /></a>&nbsp;</td> 
<td align="left"><p>PhD Candidate<br />
 ←←←←←←← My very cute~cute~cute~cute~cute daughter & I   <br />
<a href="https://">Multimedia Processing Laboratory (MP Lab)</a>, <br />
<a href="https://dcs.site.nthu.edu.tw/">Department of Computer Science</a>,  <br />
<a href="https://www.nthu.edu.tw/">National Tsing Hua University</a>, <br />
101, Section 2, Kuang-Fu Road, Hsinchu, Taiwan<br />
Postcode: 30013 <br />
Email: <a href= "mailto:alwayswithme@gapp.nthu.edu.tw">alwayswithme@gapp.nthu.edu.tw</a>  <br />
<br />
<a href="https://scholar.google.com/citations?user=T0hELCEAAAAJ&hl=en">[Google Scholar]</a> 
  <a href="https://github.com/Pei-KaiHuang">[GitHub]
	 <a href="https://dblp.org/pid/320/2231.html">[dblp]
</td></tr></table>
<h2>About me</h2> 
<p>
   I will return to my alma mater, Fujian Normal University, for a faculty position.   
   I've received the bachelor degree from Fujian Normal University in 2017, the master degree from National Central University in 2019, and the Ph.D. degree from National Tsing Hua University in 2025.
   I live in Xiamen, China. 
</p>	 
<p>My research area focuses on face anti-spoofing and rPPG Estimation. As the first author, I have been published in prestigious conferences in the field of computer vision, including CVPR, AAAI, BMVC, ICME, ICIP, and ACPR, as well as in renowned journals, including TIFS, Information Science, and ATSIP.  </p>
<p> I have reviewed papers from different CV/AI conferences, e.g., CVPR, ECCV, AAAI, ACM MM, ICME, BMVC, and ICIP, and biomedical related journals, e.g., TIFS, TIM, TBME, BSPC. </p>
<p> <b>If you are interested in pursuing a PhD at National Tsinghua University, National Taiwan University, National Yang Ming Chiao Tung University, National Cheng Kung University, National Central University, please contact me. I would be happy to introduce you to a professor.
</b>
</p>
</td></tr></table>

<a name="Preprint"><h3>News</h3></a> 
<hr /> 
[2025/04] One paper was accepted by IEEE Transactions on Information Forensics & Security (TIFS) <br>
[2025/01] One paper was accepted by Information Sciences <br>
[2024/12] Offered a faculty position at Fujian Normal University <br>
[2024/12] One paper was accepted by AAAI 2025<br>	
[2024/02] One paper was accepted by CVPR 2024<br>	
[2023/08] One paper was accepted by BMVC 2023<br>	
[2023/06] Two papers were accepted by ICIP 2023<br>	
[2023/03] One paper was accepted by ICME 2023<br>
[2022/10] One paper was accepted by BMVC 2022<br>	
[2022/03] One paper was accepted by ICME 2022<br>	
[2021/08] One paper was accepted by ACPR 2021 <br>	


<a name="Preprint"><h3>Preprint</h3></a> 
	<hr />
<!-- paper J04 -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2025PR_rPPG_TTA"><img src="picture/papers/2025PR_rPPG_TTA.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>Fully Test-Time rPPG Estimation via Synthetic Signal-Guided Feature Learning</i>
			<br>
		    <b>Pei-Kai Huang</b>, Tzu-Hsien Chena, Ya-Ting Chana, Kuan-Wen Chena, Shih-Yu Yanga, Yen-Chun Choua, Chiou-Ting Hsu<br>
	  
		       Pattern Recognition, (Major revision) , [中科院一区Top, CCF B,<a href="https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf" style="color:#000000;">TH-CPL Rank B</a>, SCIE:Q1, IF:7.5, h-index:180]
			<br>
		  <a href="https://arxiv.org/pdf/2407.13322" style="color:#000000;">Paper</a>  |   Code 
			
		</div>
	</td>
	</tr></tbody></table> 
 <br>  
	
<!-- paper J03 -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2025PR_LDCformer"><img src="picture/papers/2025_PR_LDCformer.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>Enhancing Learnable Descriptive Convolutional Vision Transformer for Face Anti-Spoofing</i>
			<br>
		    <b>Pei-Kai Huang</b>, Jun-Xiong Chong, Ming-Tsung Hsu,  Fang-Yu Hsu, Chiou-Ting Hsu<br>
	  
		       Pattern Recognition, (Major revision) , [中科院一区Top, CCF B,<a href="https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf" style="color:#000000;">TH-CPL Rank B</a>, SCIE:Q1, IF:7.5, h-index:180]
			<br>
		  <a href="https://arxiv.org/pdf/2503.22936" style="color:#000000;">Paper</a>  |  
		 <a href="https://github.com/Pei-KaiHuang/LDCformer_Ext" style="color:#000000;">Code</a>
	
		</div>
	</td>
	</tr></tbody></table> 
 <br>  
 


<!-- paper JXX -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2025UFDANet"><img src="picture/papers/2025_UFDANet.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>Unsupervised Feature Disentanglement and Augmentation Network for One-class Face Anti-spoofing</i>
			<br>
		    <b>Pei-Kai Huang</b>, Jun-Xiong Chong, Ming-Tsung Hsu, Fang-Yu Hsu, Yi-Ting Lin, Kai-Heng Chien, Hao-Chiang Shao, Chiou-Ting Hsu<br>
	  
		       Under review
			<br>
		   <a href="https://arxiv.org/pdf/2503.22929" style="color:#000000;">Paper</a>  | 
			 Code  
			
		</div>
	</td>
	</tr></tbody></table> 
 <br>  
	

	<a name="publication"><h3>Publications (Journal)</h3></a> 
	<hr />
<!-- paper J04 -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2025PR_LDCformer"><img src="picture/papers/2025_TIFS_DDrPPG.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>DD-rPPGNet: De-interfering and Descriptive Feature Learning for Unsupervised rPPG Estimation</i>
			<br>
		    <b>Pei-Kai Huang</b>, Tzu-Hsien Chen, Ya-Ting Chan, Kuan-Wen Chen, Chiou-Ting Hsu<br>
	  
		       IEEE Transactions on Information Forensics & Security (TIFS), (Accepted) , [中科院一区Top, CCF A,<a href="https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf" style="color:#000000;">TH-CPL Rank A</a>, SCIE:Q1, IF:6.3, h-index:95]
			<br>
		   <a href="https://arxiv.org/pdf/2407.21402" style="color:#000000;">Paper</a>  | 
			<a href="https://github.com/Pei-KaiHuang/TIFS2025-DD-rPPGNet" style="color:#000000;">Code</a> 
			 
			
		</div>
	</td>
	</tr></tbody></table> 
 <br>  
	
	
<!-- paper J02 -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2024IS_CDformer"><img src="picture/papers/2024_Information Science.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>Channel difference transformer for face anti-spoofing</i>
			<br>
		    <b>Pei-Kai Huang</b>, Jun-Xiong Chong, Ming-Tsung Hsu,  Fang-Yu Hsu, Chiou-Ting Hsu<br>
	  
		       Information Sciences, 2025 [中科院二区, CCF B,<a href="https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf" style="color:#000000;">TH-CPL Rank B</a> , SCIE:Q1]
			<br>
		  <a href="https://www.sciencedirect.com/science/article/pii/S0020025525000362" style="color:#000000;">Paper</a>    | 
		<a href="https://github.com/Pei-KaiHuang/CDformer" style="color:#000000;">Code</a> 
		    
		</div>
	</td>
	</tr></tbody></table> 
 <br> 
<!-- paper J01 -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2024_ATSIP.png"><img src="picture/papers/2024_ATSIP.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>A Survey on Deep Learning-based Face Anti-Spoofing</i>
			<br>
		    <b>Pei-Kai Huang</b>, Jun-Xiong Chong, Ming-Tsung Hsu,  Fang-Yu Hsu, Cheng-Hsuan Chiang, Tzu-Hsien Chen,  Chiou-Ting Hsu<br>
	  
		      APSIPA Transactions on Signal and Information Processing, 2024 [中科院三区, ESCI:Q1, IF:3.2, h-index:25]
			<br>
		  <a href="https://www.nowpublishers.com/article/OpenAccessDownload/SIP-20240053" style="color:#000000;">Paper</a>    
		    
		</div>
	</td>
	</tr></tbody></table> 
 <br> 



	
	<a name="publication"><h3>Publications (Conference)</h3></a> 
	<hr /> 
<!-- paper C09 -->
<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2025AAAI"><img src="picture/papers/2025_AAAI.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>SLIP: Spoof-aware one-class face anti-spoofing with Language Image Pretraining</i>
			<br>
		    <b>Pei-Kai Huang</b>, Jun-Xiong Chong, Cheng-Hsuan Chiang, Tzu-Hsien Chen, Tyng-Luh Liu, Chiou-Ting Hsu.<br>
	  
		     The 39th Annual AAAI Conference on Artificial Intelligence (AAAI), 2025, USA. , [<a href="-" style="color:#000000;">CORE Rank A*</a>,
			CCF A, <a href="https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf" style="color:#000000;">TH-CPL Rank A</a> ]
			<br>
		 <a href="https://arxiv.org/pdf/2503.19982" style="color:#000000;">Paper</a>   | 
		<a href="https://github.com/Pei-KaiHuang/AAAI25-SLIP" style="color:#000000;">Code</a> |
         <a download href="https://github.com/Pei-KaiHuang/Pei-KaiHuang.github.io/blob/main/poster/2025_AAAI_poster.pdf" style="color:#000000;" >Poster</a>		 
		</div>
	</td>
	</tr></tbody></table> 
 <br> 
<!-- paper 8 -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2024CVPR"><img src="picture/papers/2024_CVPR.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>One-Class Face Anti-spoofing via Spoof Cue Map-Guided Feature Learning</i>
			<br>
		    <b>Pei-Kai Huang</b>,  Cheng-Hsuan Chiang,  Tzu-Hsien Chen, Jun-Xiong Chong, Tyng-Luh Liu, Chiou-Ting Hsu<br>
	  
		      The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, Seattle WA, USA.</a>, [h5-index:422,  
			 <a href="-" style="color:#000000;">CORE Rank A*</a>,
			CCF A, <a href="https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf" style="color:#000000;">TH-CPL Rank A</a> ]
			<br>
		  <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_One-Class_Face_Anti-spoofing_via_Spoof_Cue_Map-Guided_Feature_Learning_CVPR_2024_paper.pdf" style="color:#000000;">Paper</a>  | 
		 <a href="https://github.com/Pei-KaiHuang/CVPR24_OC_SCMNet" style="color:#000000;">Code</a> |
		  <a href="https://www.bilibili.com/video/BV1Lw4m1Q7fK/?share_source=copy_web&vd_source=6a5816aefc89c8d16c3700c5633c43a9" style="color:#000000;">Video</a> |
			<a download href="https://github.com/Pei-KaiHuang/Pei-KaiHuang.github.io/blob/main/poster/2024_CVPR_poster.pdf" style="color:#000000;" >Poster</a>	  
		</div>
	</td>
	</tr></tbody></table> 
 <br> 

 <!-- paper 7 -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2022BMVC"><img src="picture/papers/2023_BMVC.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>Test-Time Adaptation for Robust Face Anti-Spoofing</i>
			<br>
		    <b>Pei-Kai Huang</b>, Chen-Yu Lu,  Shu-Jung Chang, Jun-Xiong Chong, Chiou-Ting Hsu<br>
	  
		       British Machine Vision Conference (BMVC), 2023, Aberdeen, UK</a>, [h5-index:75,  
			 <a href="http://portal.core.edu.au/conf-ranks/?search=BMVC&by=all&source=CORE2023&sort=atitle&page=1" style="color:#000000;">CORE Rank A</a>,
			CCF C, <a href="https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf" style="color:#000000;">TH-CPL Rank B</a> ]
			<br>
		 <a href="https://papers.bmvc2023.org/0379.pdf" style="color:#000000;">Paper</a> | 
		 <a href="https://github.com/Pei-KaiHuang/TTA-FAS" style="color:#000000;">Code</a> | 
		   <a href="https://www.bilibili.com/video/BV1fN4y1r7yL/?share_source=copy_web&vd_source=6a5816aefc89c8d16c3700c5633c43a9" style="color:#000000;">Video</a>  
			| <a download href="https://github.com/Pei-KaiHuang/Pei-KaiHuang.github.io/blob/main/poster/2023_BMVC_poster.pdf" style="color:#000000;" >Poster</a>
		</div>
	</td>
	</tr></tbody></table> 
 <br> 
  

	
 <!-- paper 6 -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2023_ICIP_FAS"><img src="picture/papers/2023_ICIP_FAS.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i> LDCformer: Incorporating Learnable Descriptive Convolution to Vision Transformer for Face Anti-Spoofing </i>
			<br> 
	      <b>Pei-Kai Huang</b>, Cheng-Hsuan Chiang, Jun-Xiong Chong, Tzu-Hsien Chen, Hui-Yu Ni, Chiou-Ting Hsu <br> 
		  IEEE International Conference on Image Processing (ICIP), 2023, Kuala Lumpur, Malaysia</a>,
                 [h5-index:60,
		 <a href="http://portal.core.edu.au/conf-ranks/?search=ICIP&by=all&source=CORE2023&sort=atitle&page=1" style="color:#000000;">CORE Rank B</a>,
			CCF C, <a href="https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf" style="color:#000000;">TH-CPL Rank B</a> ]
			<br>
		 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10222330" style="color:#000000;">Paper</a> |
		 <a href="https://github.com/Pei-KaiHuang/ICIP23_D-LDCformer" style="color:#000000;">Code</a> |
		  <a href="https://www.bilibili.com/video/BV1Wh4y1M75d/?share_source=copy_web&vd_source=6a5816aefc89c8d16c3700c5633c43a9" style="color:#000000;">Video</a>   
		| <a download href="https://github.com/Pei-KaiHuang/Pei-KaiHuang.github.io/blob/main/poster/2023_ICIP_poster.pdf" style="color:#000000;" >Poster</a>
		</div>
	</td>
	</tr></tbody></table> 
 <br>   	
	
	
 <!-- paper 5 -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2023ICIP_Seg"><img src="picture/papers/2023_ICIP_Seg.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>  Single-Domain Generalization for Semantic Segmentation via Dual-level Domain Augmentation </i>
			<br>
		   Shu-Jung Chang*, Chen-Yu Lu*,  <b>Pei-Kai Huang*</b>, Chiou-Ting Hsu (* equal contribution)<br> 
	  
		 IEEE International Conference on Image Processing (ICIP), 2023, Kuala Lumpur, Malaysia</a>, 
			[h5-index:60, 
			 <a href="http://portal.core.edu.au/conf-ranks/?search=ICIP&by=all&source=CORE2023&sort=atitle&page=1" style="color:#000000;">CORE Rank B</a>,
			CCF C, <a href="https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf" style="color:#000000;">TH-CPL Rank B</a> ]
			<br>
		 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10222684" style="color:#000000;">Paper</a> |
		  <a href="https://www.bilibili.com/video/BV1Nw411U7Qv/?share_source=copy_web&vd_source=6a5816aefc89c8d16c3700c5633c43a9" style="color:#000000;">Video</a>   
		 | Oral	  
		</div>
	</td>
	</tr></tbody></table> 
 <br>   	
	
 <!-- paper 4 -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2023ICME"><img src="picture/papers/2023_ICME.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>Towards Diverse Liveness Feature Representation and Domain Expansion for Cross-Domain Face Anti-Spoofing </i>
			<br>
		    <b>Pei-Kai Huang</b>, Jun-Xiong Chong, Hui-Yu Ni, Tzu-Hsien Chen, Chiou-Ting Hsu<br>
	  
		   IEEE International Conference on Multimedia & Expo (ICME), 2023, Brisbane, Australia</a>, [h5-index:40,
			 <a href="http://portal.core.edu.au/conf-ranks/?search=ICME&by=all&source=CORE2023&sort=atitle&page=1" style="color:#000000;">CORE Rank A</a>,
			CCF B]
			<br>
		  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10220051" style="color:#000000;">Paper</a> | 
		  <a href="https://github.com/Jxchong1999/DFANet" style="color:#000000;">Code</a> | 
		 <a href="https://www.bilibili.com/video/BV1Y8411D7DA/?share_source=copy_web&vd_source=6a5816aefc89c8d16c3700c5633c43a9" style="color:#000000;">Video</a>  | 
		  <a download href="https://github.com/Pei-KaiHuang/Pei-KaiHuang.github.io/blob/main/poster/2023_ICME_poster.pdf" style="color:#000000;" >Poster</a> 
			  
		</div>
	</td>
	</tr></tbody></table> 
 <br>   	
	
	
 
 <!-- paper 3 -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2022BMVC"><img src="picture/papers/2022_BMVC.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>Learnable Descriptive Convolutional Network for Face Anti-Spoofing </i>
			<br>
		    <b>Pei-Kai Huang</b>, Hui-Yu Ni, Yan-Qin Ni, Chiou-Ting Hsu<br>
	  
		       British Machine Vision Conference (BMVC), 2022, London, UK</a>, [h5-index:75, 
			<a href="http://portal.core.edu.au/conf-ranks/?search=BMVC&by=all&source=CORE2023&sort=atitle&page=1" style="color:#000000;">CORE Rank A</a>,
			CCF C,   <a href="https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf" style="color:#000000;">TH-CPL Rank B</a> ]
			<br>
		  <a href="https://bmvc2022.mpi-inf.mpg.de/0239.pdf" style="color:#000000;">Paper</a> | 
		  <a href="https://github.com/huiyu8794/LDCNet" style="color:#000000;">Code</a> | 
		  <a href="https://www.bilibili.com/video/BV1Wd4y1n7rd/?share_source=copy_web&vd_source=6a5816aefc89c8d16c3700c5633c43a9" style="color:#000000;">Video</a>  |
		  <a download href="https://github.com/Pei-KaiHuang/Pei-KaiHuang.github.io/blob/main/poster/2022_BMVC_poster.pdf" style="color:#000000;" >Poster</a> 	  
		</div>
	</td>
	</tr></tbody></table> 
 <br> 
 
 
 <!-- paper 2 -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2022ICME"><img src="picture/papers/2022_ICME.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>Learning to Augment Face Presentation Attack Dataset via Disentangled Feature Learning from Limited Spoof Data  </i>
			<br>
		    <b>Pei-Kai Huang</b>, Chu-Ling Chang, Hui-Yu Ni, Chiou-Ting Hsu<br>
	  
		  IEEE International Conference on Multimedia & Expo (ICME), 2022, Taipei, Taiwan</a>, [h5-index:40,
			 <a href="http://portal.core.edu.au/conf-ranks/?search=ICME&by=all&source=CORE2023&sort=atitle&page=1" style="color:#000000;">CORE Rank A</a>,
			CCF B]
			<br>
		  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9859657" style="color:#000000;">Paper</a> | 
		  <a href="https://www.bilibili.com/video/BV11M411e7YP/?share_source=copy_web&vd_source=6a5816aefc89c8d16c3700c5633c43a9" style="color:#000000;">Video</a>  
		| Oral		  
		</div>
	</td>
	</tr></tbody></table> 
 <br>    
 
 
 
 	<!-- paper 1 -->
	<table><tbody><tr>
	<td valign="top" align="center" width="220" id="2021ACPR"><img src="picture/papers/2021_ACPR.png" width="200" height="100" border="0">
	</td>
	<td>
		<div align="left">
			<i>Face Anti-Spoofing via Robust Auxiliary Estimation and Discriminative Feature Learning</i>
			<br>
		    <b>Pei-Kai Huang</b>, Ming-Chieh Chin, Chiou-Ting Hsu<br>
	  
		   Asian Conference on Pattern Recognition (ACPR), 2021, Ieju Island, Korea</a>, [h5-index:15, indexed by EI]
			<br>
		  <a href="https://link.springer.com/chapter/10.1007/978-3-031-02375-0_33" style="color:#000000;">Paper</a> | 
		  <a href="https://www.bilibili.com/video/BV1tx4y1j7Af/?share_source=copy_web&vd_source=6a5816aefc89c8d16c3700c5633c43a9" style="color:#000000;">Video</a>  
		| Oral		
		</div>
	</td>
	</tr></tbody></table> 
 <br>    
 


<br><br>                                                                                                
	<a name="publication"><h3> </h3></a>
	<hr />

</div>

<div id="footer">
<div id="footer-text">
<br>Last updated：2025/04/21, by <a href= "mailto:alwayswithme@gapp.nthu.edu.tw">Pei-Kai Huang </a>. <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv">Pageviews:<span id="busuanzi_value_site_pv"></span></span>
</div>
</div>



</body>
</html>
